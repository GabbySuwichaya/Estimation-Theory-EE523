{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding for Lecture I \n",
    "\n",
    "by *Suwichaya Suwanwimolkul, Ph.D.*\n",
    "\n",
    "The coding exercies and examples are used as parts of  *Lecture I: What is an estimator?*  in **Estimation Theory EE2102523**. \n",
    "\n",
    "The topics covered in this exercise are: \n",
    "\n",
    "- [Sampling from Distribution](#sampling-from-distribution)\n",
    "- [Estimators](#estimators) \n",
    "    - [Sample Mean](#sample-mean-estimator)\n",
    "    - [Sample Variance](#sample-variance-estimator)\n",
    "- [Estimator Properties](#estimator-properties)\n",
    "    - [Bianess](#biasness)\n",
    "    - [MSE](#mse)\n",
    "    - [Consistency](#consistency)\n",
    "- [HW1.2](#hw12)\n",
    "    - [Biased Estimator](#biased-estimator)\n",
    "    - [Unbiased Estimator](#unbiased-estimator)\n",
    " \n",
    " \n",
    "\n",
    "Notes.\n",
    "- The examples of biased vs unbiased estimators are inspried by [Sampling \\& Estimation EP.3 by Khalel Corona](https://www.youtube.com/watch?v=pNbDigYLqSY).\n",
    "-  Don't forget to install the dependency `pip install -r requirements.txt`\n",
    "- `utils.py` contains the supplenmary implementations for each fucntion used in `Lecture1.ipynb` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Definition\n",
    "Suppose that the unknown parameter $\\theta$ is in real space, i.e., $\\theta\\in \\mathbb{R}$, we estimate the parameter $\\theta$ by **randomly sampling** $X_1, X_2, ..., X_N$ from the same distribution as $X$.\n",
    "\n",
    "During performing random sampling, we assume that \n",
    " \n",
    "- $X_1, X_2, ..., X_N$ is drawn independently from a distribution $f_x(X)$, e.g., $f_x(X) = Uniform(a,b), \\mathcal{Gauss}(\\mu, \\sigma^2)$.   \n",
    "- $X_1, X_2, ..., X_N$ is assumed to have the same distribution as $X$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "In the following example, we will show what does it look like when we sample $X_1, X_2, ..., X_N$  from $X$ that has a uniform distribution, Uniform($a$,$b$) where $a$ = 0, $b$ =1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Normally Distributed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import joypy\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "data_population = np.random.randn(5000)\n",
    "\n",
    "#print(\"Mean of population: %.2f\" % np.mean(data_population))\n",
    "#print(\"Variance of population: %.2f\" % np.var(data_population))\n",
    "\n",
    "textstr_original = '\\n'.join([  \n",
    "    'Original population:',\n",
    "    \"Empirical Mean: %.2f\"  % np.mean(data_population), \n",
    "    \"Empirical std: %.2f\" % np.std(data_population)])\n",
    "\n",
    "plt.hist(data_population, bins = np.arange(-7.5, 7.5, 0.1), density=True, color=\"blue\", label=\"Distribution of Y (original)\")\n",
    "plt.ylabel(\"Freq. (Density)\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.title(\"Normal Distribution\")\n",
    "plt.text(-8, 0.485, textstr_original,horizontalalignment='left', verticalalignment='top', family='monospace', color=\"blue\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0,0.5)\n",
    "plt.grid(  which='both', color='grey', linestyle=':')\n",
    "plt.savefig(\"Original_Normally_Distributed.png\",\n",
    "               bbox_inches='tight', \n",
    "               transparent=True,\n",
    "               pad_inches=0,dpi=400)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_select = 2500\n",
    "data_sampled = np.random.choice(data_population, N_select, replace=False)\n",
    " \n",
    "\n",
    "textstr_sampled = '\\n'.join([  \n",
    "    'Sampled data:',\n",
    "    \"Empirical Mean: %.2f\"  % np.mean(data_sampled), \n",
    "    \"Empirical std: %.2f\" % np.std(data_sampled)])\n",
    "\n",
    "plt.hist(data_population, bins = np.arange(-7.5, 7.5, 0.1),  density=True, color=\"blue\",  label=\"Distribution of Y (original)\")\n",
    "plt.hist(data_sampled, bins = np.arange(-7.5, 7.5, 0.1), alpha=0.75, density=True, color=\"orange\", label=\"Sampled\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel(\"Freq. (Density)\")\n",
    "plt.text(-8, 0.485, textstr_original,horizontalalignment='left', verticalalignment='top', family='monospace', color=\"blue\")\n",
    "plt.text(-8, 0.4,   textstr_sampled, horizontalalignment='left', verticalalignment='top', family='monospace', color=\"red\")\n",
    "plt.xlabel(\"Samples\") \n",
    "plt.ylim(0,0.5)\n",
    "plt.title(\"Sampled Data ($N=%d$)\" % N_select)\n",
    "plt.grid(  which='both', color='grey', linestyle=':')\n",
    "plt.savefig(\"N%d_Sampled_Data_from_Normal.png\" % N_select,\n",
    "               bbox_inches='tight', \n",
    "               transparent=True,\n",
    "               pad_inches=0,dpi=400)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Uniformly Distributed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_population = 1*np.linspace(0,1,5000)\n",
    " \n",
    "\n",
    "textstr_original = '\\n'.join([  \n",
    "    'Original population:',\n",
    "    \"Empirical Mean: %.2f\"  % np.mean(data_population), \n",
    "    \"Empirical std: %.2f\" % np.std(data_population)])\n",
    "\n",
    "plt.hist(data_population, bins = np.arange(0, 1 + 0.05, 0.05), density=True, color=\"blue\",  label=\"Distribution of Y (original)\")\n",
    "plt.text(-0, 1.375, textstr_original, horizontalalignment='left', verticalalignment='top', family='monospace', color=\"blue\")\n",
    "plt.ylabel(\"Freq. (Density)\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.title(\"Uniform Distribution\")\n",
    "plt.ylim(0,1.4)\n",
    "plt.xlim(-.05,1.05) \n",
    "plt.legend(loc='upper right')\n",
    "plt.grid( which='both', color='grey', linestyle=':')\n",
    "plt.savefig(\"Original_Uniformly_Distributed.png\",\n",
    "               bbox_inches='tight', \n",
    "               transparent=True,\n",
    "               pad_inches=0,dpi=400)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_select = 5\n",
    "data_sample = np.random.choice(data_population, N_select, replace=False)\n",
    "\n",
    "textstr_sampled = '\\n'.join([  \n",
    "    'Sampled data:',\n",
    "    \"Empirical Mean: %.2f\"  % np.mean(data_sample), \n",
    "    \"Empirical std: %.2f\" % np.std(data_sample)])\n",
    "\n",
    "plt.hist(data_population, bins = np.arange(0, 1 + 0.05, 0.05),  density=True, color=\"blue\", label=\"Distribution of Y (orange)\")\n",
    "plt.hist(data_sample, bins = np.arange(0, 1 + 0.05, 0.05), alpha=0.75, density=True, color=\"orange\", label=\"Sampled data\") \n",
    "\n",
    "plt.text(-0, 1.375,   textstr_original, horizontalalignment='left', verticalalignment='top', family='monospace', color=\"blue\")\n",
    "plt.text(-0, 1.185,   textstr_sampled, horizontalalignment='left', verticalalignment='top', family='monospace', color= \"red\")\n",
    "\n",
    "plt.ylabel(\"Freq. (Density)\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylim(0,1.4)\n",
    "plt.xlim(-.05,1.05)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Sampled Data ($N=%d$)\" % N_select)\n",
    "plt.grid(  which='both', color='grey', linestyle=':')\n",
    "plt.savefig(\"N%d_Sampled_Data_from_Uniform.png\" % N_select,\n",
    "               bbox_inches='tight', \n",
    "               transparent=True,\n",
    "               pad_inches=0,dpi=400)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import sample_mean_simulation, sample_variance_simulation, hist_plot, bias_func, MSE_func\n",
    "\n",
    "# Here we just import all the tools from `utils.py` \n",
    "# So, please carefully read each function writen in utils.py for the implemenation details\n",
    "# \n",
    "# Below are some the examples of the functions in utils.py \n",
    "# =================================================================\n",
    "# def sample_mean_simulation(data_population, select, size = 1000):  \n",
    "#     ....  \n",
    "# def bias_func(estimates, gt): \n",
    "#   np.mean(estimates) - gt \n",
    "#    ...\n",
    "# def sample_variance_simulation .....\n",
    "#    ...  \n",
    "# def hist_plot\n",
    "#    .... so on...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Mean Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_1$, $X_2$,  ..., $X_n$ be a random sample sharing the same distribution as  $X$, but the parameters (\\eg, mean and variance) could be unknown. \n",
    " \n",
    "**Sample mean** is used as an estimator for the mean value of $X$,  is defined as \n",
    "\\begin{equation}\n",
    "    \\Theta_{\\mu} := \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Variance Estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Sample Variance Estimator\n",
    "\n",
    "Let $X_1$, $X_2$,  ..., $X_n$ be a random sample sharing the same distribution as  $X$, but the parameters ,eg, mean and variance, could be unknown. \n",
    "\n",
    "***Sample variance estimator*** for variance of $X$ is defined as \n",
    "\\begin{equation}\n",
    "    \\Theta_{\\sigma^2} := S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Biasness](#biasness)\n",
    "- [MSE](#mse)\n",
    "- [Consistency](#consistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biasness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\Theta := f(X_1, X_2, ..., X_N)$ be the point estimator for $\\theta$. The bias of the point estimator is defined as \n",
    "\\begin{equation}  \n",
    "\\text{Bias}_{\\theta}{(\\Theta)} = E(\\Theta) - \\theta.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:  Biasness of Sample Mean Estimator\n",
    "In the following example, we compute the bias of the given estimators and show \n",
    "- the histogram of estimators, \n",
    "- the mean of estimators \n",
    "- the true variance.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_name_temp =  \"sample mean\" \n",
    "\n",
    "N_select = 100 # the number of samples selected from the entire population\n",
    "\n",
    "gt = np.mean(data_population) # the true population mean\n",
    " \n",
    "sample_mean_estimates = sample_mean_simulation(data_population, N_select) # simulating the sample mean estimator (the sample mean estimates are the output).\n",
    "\n",
    "\n",
    "Bias = bias_func(sample_mean_estimates, gt) # compute the bias\n",
    "\n",
    "\n",
    "# plotting histrogram, mean of the estimators, and the true mean value of the data population \n",
    "hist_plot(sample_mean_estimates, data_population, gt, bias_func, estimator_name = \"%s @ N=%d\" % (estimator_name_temp, N_select), ploting = \"Bias\", dirpath=cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Definition:\n",
    "\n",
    "If  $\\Theta$ is an unbiased estimator of $\\theta$, ie, $E[\\Theta] = \\theta$, then the efficiency of the estimator is **inverse proportional** to $\\text{Var}{[\\Theta]}  = E[(\\Theta-\\mu_{\\Theta})^2]$. **An efficient estimator** can also be characterized by having the lower variances from the true value $\\theta$, which can be measured by mean-square error (MSE) ... \n",
    "\n",
    "**Mean squared error (MSE) of an estimator $\\Theta$**  is defined as: \n",
    "\n",
    "\\begin{equation}  \n",
    "MSE_{\\theta}(\\Theta) = E_{\\Theta} [(\\Theta - \\theta)^2].\n",
    "\\end{equation} \n",
    "\n",
    "The smaller MSE is generally indicative of the better estimator. \n",
    "\n",
    "#### Example: MSE of Sample Mean Estimators\n",
    "\n",
    "After applying the above equation to derive the MSE of sample mean $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, \n",
    "\n",
    "you will find that the MSE is actually a function of the number of samples ($n$).\n",
    "\n",
    "So, in the following example, we want you to simulate the emperical MSE at different $n$ to verify if you reach a similar conclusion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "N_select_list = np.linspace(1, 100).tolist() # Adjust the ranges of n and see the convergence of MSE\n",
    "ploting  = \"MSE\"\n",
    "MSE_list = []  \n",
    "\n",
    "gt = np.mean(data_population)\n",
    "\n",
    "for N_select in N_select_list: \n",
    "    sample_mean_estimates = sample_mean_simulation(data_population, int(N_select))    # See utils.py\n",
    "    var, bias_square = MSE_func(sample_mean_estimates, gt)   # See utils.py\n",
    "    mse              = var + bias_square\n",
    "    MSE_list.append(mse)\n",
    "    \n",
    "plt.plot(N_select_list, MSE_list)\n",
    "plt.xlabel(\"n := number of selected samples\")\n",
    "plt.ylabel(\"MSE(n)\")\n",
    "plt.title(\"MSE of %s\" % estimator_name_temp)\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(cwd, \"%s-%s.pdf\" % (ploting, estimator_name_temp)))\n",
    "plt.savefig(os.path.join(cwd, \"%s-%s.png\" % (ploting, estimator_name_temp)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Definition:\n",
    "\n",
    "Let $\\Theta_1, \\Theta_2, ..., \\Theta_n, ...,$ be a sequence of the point estimator $\\Theta$; each of which is sorted by the number of the random samples  $n$.  \n",
    " \n",
    "Here, we write an estimator as a function of $n$ random samples explicitly, ie, $\\Theta=\\Theta_n = f(X_1, X_2, ..., X_n)$.   \n",
    " \n",
    "For any $\\epsilon > 0$,  we say that $\\Theta$ is a consistent estimator of $\\theta$, if \n",
    "\\begin{equation}\n",
    "    \\lim_{n \\rightarrow \\infty} P (|\\Theta_n - \\theta | \\geq \\epsilon) = 0  \\hspace{0.5cm} \\text{for all possible value of~} \\theta.\n",
    "\\end{equation}\n",
    "\n",
    "In other words, an estimator $\\Theta$ is a consistent estimator for $\\theta$, if  \n",
    "$\\Theta$ converges to $\\theta$ in probability, or $\\Theta \\xrightarrow{p} \\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Consistency of Sample Mean Estimators\n",
    "\n",
    "In this example, we look at the density of $\\Theta_n - \\theta$ to reflect the empirical distribution of $P (\\Theta_n - \\theta) $.\n",
    "\n",
    "We want to check the following characteristics:\n",
    "\n",
    "- Does the empirical distribution of $ \\Theta_n - \\theta $ is centering at 0  ? \n",
    "- Is the shape of the distribution is getting smaller and smaller such that it can be bounded by a small positive value ($\\epsilon$) ? \n",
    "\n",
    "If the empirical distribution shows these characteristics, it suggests that the estimator converges to $\\theta$ in probability (as verified by the shape of the histrogram density). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency \n",
    "ploting  = \"Consistency\"\n",
    "gt = np.mean(data_population)\n",
    "\n",
    "epsilon = 0\n",
    "\n",
    "N_select_list = np.linspace(1, 4000, 10).tolist() # Adjust the ranges of n and see the convergence of Consistency  \n",
    "Population_list = [] \n",
    " \n",
    "df = pd.DataFrame()\n",
    "df_logics = pd.DataFrame()\n",
    "\n",
    "for N_select in N_select_list:\n",
    "    sample_mean_estimates = sample_mean_simulation(data_population, int(N_select))  # See utils.py\n",
    "    estimates_diff        = sample_mean_estimates - gt\n",
    "    df[\"N=%d\" % int(N_select)]     = estimates_diff\n",
    " \n",
    "    df_logics[\"N=%d\" % int(N_select)] = 1.0*(np.absolute(estimates_diff) > 1e-2)\n",
    "\n",
    " \n",
    "joypy.joyplot(df, overlap=2, hist=False, colormap=cm.jet, alpha=0.6, linecolor='w', linewidth=.5, xlabels=True, ylabels=True, ylim='mean')\n",
    "plt.xlabel(\"$\\Theta_N - \\mu$\") \n",
    "plt.title(\"Show  Emperical Density for $P(\\Theta_N - \\mu)$\") \n",
    "plt.show()\n",
    "\n",
    "\n",
    "joypy.joyplot(df_logics, overlap=2, hist=False, colormap=cm.jet, alpha=0.6, linecolor='w', linewidth=.5, xlabels=True, ylabels=True, ylim='mean')\n",
    "plt.xlabel(\"$|\\Theta_N - \\mu| \\geq \\epsilon$\") \n",
    "plt.title(\"Show  Emperical Density for $P(|\\Theta_N- \\mu| \\geq \\epsilon)$ \") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW.1.2 \n",
    "\n",
    "Let $X_1, X_2, ...$ be a random sample from a distribution with mean $\\mu$ and variance $\\sigma^2$. Note that an empirical variance denoted as $S^2_{\\mu}$ is given by the following equation and is not the same as the sample variance $S^2$.\n",
    "\n",
    "\\begin{equation}  \n",
    "   S^2_{\\mu} = \\frac{1}{n}  \\sum_i \\left( X_i - \\bar{X} \\right)^2  \n",
    "\\end{equation}\n",
    "\n",
    " \n",
    "- Q1. Is the empirical variance $S^2_{\\mu}$,  an unbiased estimator for variance $\\sigma^2$?    ***Hint!*** Check if  $E[S^2_{\\mu}]  - \\sigma^2 = 0$?\n",
    "\n",
    "- Q2. Is sample variance $S^2$ an unbiased estimator for variance $\\sigma^2$?   \n",
    "\n",
    "- Q3. At $n = 10, 100, 1000, 10000$ (`N_select`:= $n$), and simulate the bias of both estimators  $S^2_{\\mu}$ and $S^2$. \n",
    "    -  Write the description for each simulation at different $n$ values. \n",
    "    - Provide **two sets of  plots** for the estimators $S^2$ and $S^2_{\\mu}$, ie, the histogram of estimators, the mean of estimators, and the true variance (provided by `hist_plot`).  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbiased Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_select = 10\n",
    "sample_variance_estimates = sample_variance_simulation(data_population, N_select, unbiased = True)  # See utils.py\n",
    "gt = np.var(data_population)\n",
    "hist_plot(sample_variance_estimates, data_population, gt, bias_func, estimator_name = \"sample variance -unbiased @ N=%d\" % N_select, ploting = \"Bias\", dirpath=cwd)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_select = 10\n",
    "sample_variance_estimates = sample_variance_simulation(data_population, N_select, unbiased = False) # See utils.py\n",
    "gt = np.var(data_population)\n",
    "hist_plot(sample_variance_estimates, data_population, gt, bias_func, estimator_name = \"sample variance -biased @ N=%d\" % N_select, ploting = \"Bias\", dirpath=cwd)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
